<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>Translator</title>
 <style>
    body {
      font-family: Arial, sans-serif;
      background-color: #f0f0f0;
      margin: 0;
      padding: 20px;
    }
    
    .container {
      max-width: 800px;
      margin: 0 auto;
    }
    
    .language-selector {
      margin-bottom: 20px;
    }
    
    #language-dropdown {
      padding: 8px;
      border-radius: 4px;
      border: 1px solid #ccc;
      font-size: 16px;
      margin-left: 10px;
    }
    
    .caption-container {
      background-color: rgba(0, 0, 0, 0.7);
      border-radius: 8px;
      padding: 20px;
      margin-bottom: 20px;
      min-height: 100px;
      display: flex;
      align-items: center;
      justify-content: center;
    }
    
    .caption-text {
      color: white;
      font-size: 20px;
      text-align: center;
      line-height: 1.5;
      animation: fadeIn 0.5s ease-in-out;
    }
    
    @keyframes fadeIn {
      from { opacity: 0; }
      to { opacity: 1; }
    }
    
    .instructions {
      text-align: center;
      color: #666;
      font-style: italic;
    }
  </style>
</head>
<body>
  <div class="container">
    <div class="language-selector">
      <label for="language-dropdown">üåê</label>
      <select id="language-dropdown">
        {% for language in languages %}
          <option value="{{ language.code }}" data-waiting="{{ language.messages.waiting }}" data-switching="{{ language.messages.switching }}" data-instructions="{{ language.messages.instructions }}">{{ language.name }}</option>
        {% endfor %}
      </select>
    </div>
    
    <div class="caption-container">
      <div id="caption" class="caption-text">{{ languages[0].messages.waiting }}</div>
    </div>
    
    <div class="instructions">
      <p id="instructions-text">{{ languages[0].messages.instructions }}</p>
    </div>
  </div>
  <script>
    // Create an AudioContext
    const audioContext = new (window.AudioContext || window.webkitAudioContext)();
    let nextPlayTime = audioContext.currentTime; // time when the next audio segment should play
    const apiBaseUrl = "{{ api_base_url }}";

    // Polling interval in milliseconds (adjust as needed)
    const pollInterval = 3000;

    // Maintain last received timestamp so we only fetch new data
    let lastAudioTimestamp = 0;
    let lastCaptionTimestamp = 0;
    
    // Get language dropdown element
    const languageDropdown = document.getElementById('language-dropdown');
    const captionElement = document.getElementById("caption");
    const instructionsElement = document.getElementById("instructions-text");
    
    /**
     * Get the currently selected language
     */
    function getSelectedLanguage() {
      return languageDropdown.value;
    }

    /**
     * Poll the /audio/translated-voice endpoint for new audio segments.
     * Each segment is expected to be an object with a "timestamp" and an "audio" field (base64 encoded MP3).
     */
    async function pollAudio() {
      try {
        const language = getSelectedLanguage();
        const response = await fetch(`${apiBaseUrl}/audio/translated-voice?lang=${language}&timestamp=${lastAudioTimestamp}`);
        if (!response.ok) {
          throw new Error("Audio fetch error");
        }
        const audioSegments = await response.json();

        // Process each new segment
        for (const segment of audioSegments) {
          // Update lastAudioTimestamp to avoid fetching these segments again.
          if (segment.timestamp > lastAudioTimestamp) {
            lastAudioTimestamp = segment.timestamp;
          }
          // Convert the base64 audio data into an ArrayBuffer.
          const base64String = segment.audio;
          const binaryString = atob(base64String);
          const len = binaryString.length;
          const bytes = new Uint8Array(len);
          for (let i = 0; i < len; i++) {
            bytes[i] = binaryString.charCodeAt(i);
          }

          // Decode the audio data and schedule it for playback.
          audioContext.decodeAudioData(bytes.buffer, (decodedData) => {
            // Create a buffer source node.
            const source = audioContext.createBufferSource();
            source.buffer = decodedData;
            source.connect(audioContext.destination);

            // If nextPlayTime is in the past, adjust it to play slightly in the future for seamless playback.
            let startTime = nextPlayTime;
            const now = audioContext.currentTime;
            if (startTime < now) {
              startTime = now + 0.1;
            }
            source.start(startTime);
            // Update nextPlayTime for subsequent segments.
            nextPlayTime = startTime + decodedData.duration;
          }, (error) => {
            console.error("Error decoding audio", error);
          });
        }
      } catch (err) {
        console.error("Error fetching audio segments:", err);
      }
    }

    /**
     * Poll the /captions/translated endpoint for new captions.
     * The endpoint returns an array of caption objects, each with a "timestamp" and "text" field.
     */
    async function pollCaptions() {
      try {
        const language = getSelectedLanguage();
        const response = await fetch(`${apiBaseUrl}/captions/translated?lang=${language}&timestamp=${lastCaptionTimestamp}`);
        if (!response.ok) {
          throw new Error("Caption fetch error");
        }
        const captions = await response.json();

        // If new captions are returned, update the display.
        if (captions.length > 0) {
          // For simplicity, display the latest caption.
          const latestCaption = captions[captions.length - 1];
          lastCaptionTimestamp = latestCaption.timestamp;
          
          // Update caption with animation effect
          captionElement.style.animation = 'none';
          captionElement.offsetHeight; // Trigger reflow
          captionElement.style.animation = 'fadeIn 0.5s ease-in-out';
          captionElement.innerText = latestCaption.text;
        }
      } catch (err) {
        console.error("Error fetching captions:", err);
      }
    }

    /**
     * Set up polling for both audio and caption endpoints.
     */
    function startPolling() {
      setInterval(() => {
        pollAudio();
        pollCaptions();
      }, pollInterval);
    }
    
    /**
     * Reset timestamps when language changes and update UI text
     */
    languageDropdown.addEventListener('change', () => {
      lastAudioTimestamp = 0;
      lastCaptionTimestamp = 0;
      
      // Get the selected option
      const selectedOption = languageDropdown.options[languageDropdown.selectedIndex];
      
      // Update caption and instructions with localized text
      captionElement.innerText = selectedOption.dataset.switching;
      instructionsElement.innerText = selectedOption.dataset.instructions;
    });

    // Resume the AudioContext on user interaction (e.g., a click) since many browsers block audio autoplay.
    document.body.addEventListener('click', () => {
      if (audioContext.state === 'suspended') {
        audioContext.resume();
      }
    });

    // Start polling after the window loads.
    window.addEventListener('load', () => {
      startPolling();
    });
  </script>
</body>
</html>
